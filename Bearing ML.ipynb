{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Data Science Capstone\n",
    "# Nasa Bearings Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0rc (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2)\n",
      "ERROR: No matching distribution found for tensorflow==2.2.0rc\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.2.0rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if not tf.__version__ == '2.2.0-rc0':\n",
    "    print(tf.__version__)\n",
    "    raise ValueError('please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import all the dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sys\n",
    "from queue import Queue\n",
    "import pandas as pd\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grab the files necessary for taining. Those are sampled from the lorenz attractor model implemented in NodeRED. Those are two serialized pickle numpy arrays. In case you are interested in how these data has been generated please have a look at the following tutorial. https://developer.ibm.com/tutorials/iot-deep-learning-anomaly-detection-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm watsoniotp.*\n",
    "!wget https://raw.githubusercontent.com/Zernez/Coursera_Advanced_Capstone/main/Dataset/DataBroken_InnerBearing.csv\n",
    "!wget https://raw.githubusercontent.com/Zernez/Coursera_Advanced_Capstone/main/Dataset/DataBroken_RollerBearing.csv\n",
    "!wget https://raw.githubusercontent.com/Zernez/Coursera_Advanced_Capstone/main/Dataset/Data_Bearing4.csv\n",
    "!wget https://raw.githubusercontent.com/Zernez/Coursera_Advanced_Capstone/main/Dataset/Data_Bearing1.csv   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De-serialize the numpy array containing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fault_inner = pd.read_csv('DataBroken_InnerBearing.csv', sep=\",\", header=None, names=[\"Bearing_X\", \"Bearing_Y\"])\n",
    "df_fault_roller = pd.read_csv('DataBroken_RollerBearing.csv', sep=\",\", header=None, names=[\"Bearing_X\", \"Bearing_Y\"])\n",
    "df_bearing_4 = pd.read_csv('Data_Bearing4.csv', sep=\",\", header=None, names=[\"Bearing_X\", \"Bearing_Y\"])\n",
    "df_bearing_1 = pd.read_csv('Data_Bearing1.csv', sep=\",\", header=None, names=[\"Bearing_X\", \"Bearing_Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fault_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape to three columns and 3000 rows. In other words three vibration sensor axes and 3000 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this data is sampled from the Lorenz Attractor Model, let's plot it with a phase lot to get the typical 2-eyed plot. First for the healthy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for the broken one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous examples, we fed the raw data into an LSTM. Now we want to use an ordinary feed-forward network. So we need to do some pre-processing of this time series data\n",
    "\n",
    "A widely-used method in traditional data science and signal processing is called Discrete Fourier Transformation. This algorithm transforms from the time to the frequency domain, or in other words, it returns the frequency spectrum of the signals.\n",
    "\n",
    "The most widely used implementation of the transformation is called FFT, which stands for Fast Fourier Transformation, let’s run it and see what it returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fault_inner_fft = np.fft.fft(df_fault_inner).real\n",
    "data_fault_roller_fft = np.fft.fft(df_fault_roller).real\n",
    "data_bearing_4_fft = np.fft.fft(df_bearing_4).real\n",
    "data_bearing_1_fft = np.fft.fft(df_bearing_4).real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s first have a look at the shape and contents of the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fault_inner_fft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we notice that the shape is the same as the input data. So if we have 3000 samples, we get back 3000 spectrum values, or in other words 3000 frequency bands with the intensities.\n",
    "\n",
    "The second thing we notice is that the data type of the array entries is not float anymore, it is complex. So those are not complex numbers, it is just a means for the algorithm the return two different frequency compositions in one go. The real part returns a sine decomposition and the imaginary part a cosine. We will ignore the cosine part in this example since it turns out that the sine part already gives us enough information to implement a good classifier.\n",
    "\n",
    "But first let’s plot the two arrays to get an idea how a healthy and broken frequency spectrum differ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "size = len(data_fault_inner_fft)\n",
    "ax.plot(range(0,size), data_fault_inner_fft[:,0].real, '-', color='blue', animated = True, linewidth=1)\n",
    "ax.plot(range(0,size), data_fault_inner_fft[:,1].real, '-', color='red', animated = True, linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(14, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "size = len(data_fault_roller_fft)\n",
    "ax.plot(range(0,size), data_fault_roller_fft[:,0].real, '-', color='blue', animated = True, linewidth=1)\n",
    "ax.plot(range(0,size), data_fault_roller_fft[:,1].real, '-', color='red', animated = True, linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "size = len(data_bearing_4_fft)\n",
    "ax.plot(range(0,size), data_bearing_4_fft[:,0].real, '-', color='blue', animated = True, linewidth=1)\n",
    "ax.plot(range(0,size), data_bearing_4_fft[:,1].real, '-', color='red', animated = True, linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "size = len(data_bearing_1_fft)\n",
    "ax.plot(range(0,size), data_bearing_1_fft[:,0].real, '-', color='blue', animated = True, linewidth=1)\n",
    "ax.plot(range(0,size), data_bearing_1_fft[:,1].real, '-', color='red', animated = True, linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what we've been doing is so called feature transformation step. We’ve transformed the data set in a way that our machine learning algorithm – a deep feed forward neural network implemented as binary classifier – works better. So now let's scale the data to a 0..1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleData(data):\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    return scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And please don’t worry about the warnings. As explained before we don’t need the imaginary part of the FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fault_inner_scaled = scaleData(data_fault_inner_fft)\n",
    "data_fault_roller_scaled = scaleData(data_fault_roller_fft)\n",
    "data_bearing_4_scaled = scaleData(data_bearing_4_fft)\n",
    "data_bearing_1_scaled = scaleData(data_bearing_1_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fault_inner_scaled = data_fault_inner_scaled.T\n",
    "data_fault_roller_scaled = data_fault_roller_scaled.T\n",
    "data_bearing_4_scaled = data_bearing_4_scaled.T\n",
    "data_bearing_1_scaled = data_bearing_1_scaled.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fault_inner_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fault_roller_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bearing_4_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bearing_1_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reshape again to have three examples (rows) and 3000 features (columns). It's important that you understand this. We have turned our initial data set which containd 3 columns (dimensions) of 3000 samples. Since we applied FFT on each column we've obtained 3000 spectrum values for each of the 3 three columns. We are now using each column with the 3000 spectrum values as one row (training example) and each of the 3000 spectrum values becomes a column (or feature) in the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fault_inner_scaled.reshape(2, 18000)\n",
    "data_fault_roller_scaled.reshape(2, 18000)\n",
    "data_bearing_4_scaled.reshape(2, 18000)\n",
    "data_bearing_1_scaled.reshape(2, 18000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Assignment\n",
    "\n",
    "The first thing we need to do is to install a little helper library for submitting the solutions to the coursera grader:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please specify you email address you are using with cousera here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task\n",
    "\n",
    "Given, the explanation above, please fill in the following two constants in order to make the neural network work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "dim = 18000\n",
    "samples = 18000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "Now it’s time to submit your first solution. Please make sure that the secret variable contains a valid submission token. You can obtain it from the courser web page of the course using the grader section of this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe how training works we just print the loss during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        sys.stdout.write(str(logs.get('loss'))+str(', '))\n",
    "        sys.stdout.flush()\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "lr = LossHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Please fill in the following constants to properly configure the neural network. For some of them you have to find out the precise value, for others you can try and see how the neural network is performing at a later stage. The grader only looks at the values which need to be precise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_neurons_layer1 = 1\n",
    "number_of_neurons_layer2 = 1\n",
    "number_of_neurons_layer3 = 1\n",
    "number_of_epochs = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Now it’s time to create the model. Please fill in the placeholders. Please note since this is only a toy example, we don't use a separate corpus for training and testing. Just use the same data for fitting and scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "from tensorflow.keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, clipnorm=1.)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(number_of_neurons_layer1,input_shape=(dim, ), activation='relu'))\n",
    "model.add(Dense(number_of_neurons_layer2, activation='relu'))\n",
    "model.add(Dense(number_of_neurons_layer3, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "\n",
    "def train(data,label):\n",
    "    model.fit(data, label, epochs=number_of_epochs, batch_size=72, validation_data=(data, label), verbose=0, shuffle=True,callbacks=[lr])\n",
    "\n",
    "def score(data):\n",
    "    return model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the training data by concatenating a label “0” for the broken and a label “1” for the healthy data. Finally we union the two data sets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_healthy = np.repeat(2,2)\n",
    "label_healthy.shape = (2,1)\n",
    "label_broken_inner = np.repeat(0,2)\n",
    "label_broken_inner.shape = (2,1)\n",
    "label_broken_roller = np.repeat(1,2)\n",
    "label_broken_roller.shape = (2,1)\n",
    "\n",
    "train_healthy = np.hstack((data_bearing_1_scaled,label_healthy))\n",
    "train_broken_inner = np.hstack((data_fault_inner_scaled,label_broken))\n",
    "train_both = np.vstack((train_healthy,train_broken))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a look at the two training sets for broken and healthy and at the union of them. Note that the last column is the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_healthy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_broken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So those are frequency bands. Notice that although many frequency bands are having nearly the same energy, the neural network algorithm still can work those out which are significantly different. \n",
    "\n",
    "## Task\n",
    "\n",
    "Now it’s time to do the training. Please provide the first 3000 columns of the array as the 1st parameter and column number 3000 containing the label as 2nd parameter. Please use the python array slicing syntax to obtain those. \n",
    "\n",
    "The following link tells you more about the numpy array slicing syntax\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_both[:,0:18000]\n",
    "labels = train_both[:,18000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to do the training. You should see the loss trajectory go down, we will also plot it later. Note: We also could use TensorBoard for this but for this simple scenario we skip it. In some rare cases training doesn’t converge simply because random initialization of the weights caused gradient descent to start at a sub-optimal spot on the cost hyperplane. Just recreate the model (the cell which contains *model = Sequential()*) and re-run all subsequent steps and train again\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(features,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=100, facecolor='w', edgecolor='k')\n",
    "size = len(lr.losses)\n",
    "ax.plot(range(0,size), lr.losses, '-', color='blue', animated = True, linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s examine whether we are getting good results. Note: best practice is to use a training and a test data set for this which we’ve omitted here for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(data_bearing_1_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(data_fault_inner_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
